# Results

## Test environment

NGINX Plus: true

NGINX Gateway Fabric:

- Commit: e4eed2dad213387e6493e76100d285483ccbf261
- Date: 2025-10-17T14:41:02Z
- Dirty: false

GKE Cluster:

- Node count: 3
- k8s version: v1.33.5-gke.1080000
- vCPUs per node: 2
- RAM per node: 4015668Ki
- Max pods per node: 110
- Zone: europe-west2-a
- Instance Type: e2-medium

## Summary:

- Total of 5 502s observed across the 4 days of the test run
- The increase in memory usage for NGF seen in the previous test run appears to have resolved.
- We observe a steady increase in NGINX memory usage over time which could indicate a memory leak.
- CPU usage remained consistant with past results. 
- Errors seem to be related to cluster upgrade or some other external factor (excluding the resolved inferences pool status error).

## Key Metrics

### Containers memory

![plus-memory.png](oss-memory.png)

### Containers CPU

![plus-cpu.png](oss-cpu.png)

## Traffic

HTTP:

```text
Running 5760m test @ http://cafe.example.com/coffee
  2 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   203.71ms  108.67ms   2.00s    66.92%
    Req/Sec   257.95    167.36     1.44k    63.57%
  173901014 requests in 5760.00m, 59.64GB read
  Socket errors: connect 0, read 219, write 55133, timeout 27
  Non-2xx or 3xx responses: 4
Requests/sec:    503.19
Transfer/sec:    180.96KB
```

HTTPS:

```text
Running 5760m test @ https://cafe.example.com/tea
  2 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   203.89ms  108.72ms   1.89s    66.92%
    Req/Sec   257.52    167.02     1.85k    63.64%
  173632748 requests in 5760.00m, 58.61GB read
  Socket errors: connect 7206, read 113, write 0, timeout 0
  Non-2xx or 3xx responses: 1
Requests/sec:    502.41
Transfer/sec:    177.84KB
```


## Error Logs

### nginx-gateway

msg: Failed to update lock optimistically: the server was unable to return a response in the time allotted, but may still be processing the request (put leases.coordination.k8s.io ngf-longevity-nginx-gateway-fabric-leader-election), falling back to slow path -> same leader election error as on oss, seems out of scope of our product

msg: Get "https://34.118.224.1:443/apis/gateway.networking.k8s.io/v1beta1/referencegrants?allowWatchBookmarks=true&resourceVersion=1760806842166968999&timeout=10s&timeoutSeconds=435&watch=true": context canceled -> possible cluster upgrade?

msg: no matches for kind "InferencePool" in version "inference.networking.k8s.io/v1" -> Thousands of these, but fixed in PR 4104

### nginx

Traffic: 5 502s

```
INFO 2025-10-19T00:12:04.220541710Z [resource.labels.containerName: nginx] 10.154.15.240 - - [19/Oct/2025:00:12:04 +0000] "GET /coffee HTTP/1.1" 502 150 "-" "-"
INFO 2025-10-19T18:38:18.651520548Z [resource.labels.containerName: nginx] 10.154.15.240 - - [19/Oct/2025:18:38:18 +0000] "GET /coffee HTTP/1.1" 502 150 "-" "-"
INFO 2025-10-20T21:49:05.008076073Z [resource.labels.containerName: nginx] 10.154.15.240 - - [20/Oct/2025:21:49:04 +0000] "GET /tea HTTP/1.1" 502 150 "-" "-"
INFO 2025-10-21T06:43:10.256327990Z [resource.labels.containerName: nginx] 10.154.15.240 - - [21/Oct/2025:06:43:10 +0000] "GET /coffee HTTP/1.1" 502 150 "-" "-"
INFO 2025-10-21T12:13:05.747098022Z [resource.labels.containerName: nginx] 10.154.15.240 - - [21/Oct/2025:12:13:05 +0000] "GET /coffee HTTP/1.1" 502 150 "-" "-"
```

No other errors identified in this test run.
